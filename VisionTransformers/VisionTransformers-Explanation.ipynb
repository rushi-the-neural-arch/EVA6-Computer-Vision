{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\r\n",
                "import torch.nn as nn\r\n",
                "\r\n",
                "# We will start with the most important part first\r\n",
                "\r\n",
                "class PatchEmbedding(nn.Module):\r\n",
                "    ''' Split Image into patches and then embed them\r\n",
                "    \r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    img_size : int\r\n",
                "        size of the image (square)\r\n",
                "\r\n",
                "    patch_size : int\r\n",
                "        size of the patch (square)\r\n",
                "\r\n",
                "    in_chans : int\r\n",
                "        Number of input channels\r\n",
                "\r\n",
                "    embed_dim : int\r\n",
                "        The embedding dimension\r\n",
                "\r\n",
                "\r\n",
                "    Attributes\r\n",
                "    ----------\r\n",
                "    n_patches : int\r\n",
                "        Number of patches inside our image\r\n",
                "\r\n",
                "    proj : nn.Conv2d\r\n",
                "        Conv layer that does both the splitting into patches and their embedding\r\n",
                "    '''\r\n",
                "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\r\n",
                "\r\n",
                "        super().__init__()\r\n",
                "        self.img_size = img_size \r\n",
                "        self.patch_size = patch_size\r\n",
                "        self.n_patches = (img_size // patch_size) ** 2\r\n",
                "\r\n",
                "        self.proj = nn.Conv2d(\r\n",
                "                in_chans,\r\n",
                "                embed_dim,\r\n",
                "                kernel_size=patch_size,\r\n",
                "                stride=patch_size\r\n",
                "        )\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        ''' Run forward pass\r\n",
                "        \r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        x : torch.tensor\r\n",
                "            Shape '(n_samples, in_chans, img_size, img_size)'\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        torch.tensor\r\n",
                "            Shape '(n_samples, n_patches, embed_dim)'  Returns a shape 3 tensor\r\n",
                "        '''\r\n",
                "\r\n",
                "        x = self.proj(\r\n",
                "                x\r\n",
                "        )  # (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\r\n",
                "\r\n",
                "        x = x.flatten(2) # (n_samples, embed_dim, n_patches)\r\n",
                "\r\n",
                "        x = x.transpose(1, 2) # (n_samples, n_patches, embed_dim)\r\n",
                "\r\n",
                "        return x"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class Attention(nn.Module):\r\n",
                "    ''' Attention mechanism\r\n",
                "    \r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    dim : int\r\n",
                "        The input and output dimension of per token features\r\n",
                "\r\n",
                "    n_heads : int\r\n",
                "        Number of attention heads\r\n",
                "\r\n",
                "    qkv_bias : bool\r\n",
                "        If True, then we include bias to the query, key and value tensors\r\n",
                "\r\n",
                "    attn_p : float\r\n",
                "        Dropout probability applied to the query, key and value tensors\r\n",
                "\r\n",
                "    proj_p : float\r\n",
                "        Dropout probability applied to the output tensor\r\n",
                "\r\n",
                "    Attributes\r\n",
                "    ----------\r\n",
                "    scale : float\r\n",
                "        Normalising constant for the dot product\r\n",
                "    \r\n",
                "    qkv : nn.Linear\r\n",
                "        Linear projection for the query, key and value\r\n",
                "\r\n",
                "    proj : nn.Linear\r\n",
                "        Linear mapping that takes in the concatenated output of all attention heads and maps it into a new space\r\n",
                "    \r\n",
                "    attn_drop, proj_drop : nn.Dropout\r\n",
                "        Dropout layers\r\n",
                "    '''\r\n",
                "\r\n",
                "    def __init__(self, dim, n_heads = 12, qkv_bias=True, attn_p = 0, proj_p = 0):\r\n",
                "        super().__init__()\r\n",
                "\r\n",
                "        self.n_heads = n_heads\r\n",
                "        self.dim = dim\r\n",
                "        self.head_dim = dim // n_heads\r\n",
                "        self.scale = self.head_dim ** -0.5\r\n",
                "\r\n",
                "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\r\n",
                "        self.attn_drop = nn.Dropout(attn_p)\r\n",
                "        self.proj = nn.Linear(dim, dim)\r\n",
                "        self.proj_drop = nn.Dropout(proj_p)\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        ''' Run forward pass\r\n",
                "        \r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        x : torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, dim)'     # dim = embedding dimensions that we receive from the above PatchEmbedding Class\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, dim)\r\n",
                "\r\n",
                "        NOTE - n_patches + 1, the extra +1 here comes from the class token that is being added every time\r\n",
                "        '''                \r\n",
                "        \r\n",
                "        n_samples, n_tokens, dim = x.shape    # n_samples = n_samples, n_tokens = n_patches + 1, dim = embedding_dimension (768)\r\n",
                "\r\n",
                "        if dim != self.dim:\r\n",
                "            raise ValueError \r\n",
                "\r\n",
                "        qkv = self.qkv(x)   # (n_samples, n_patches + 1, 3*dim)     \r\n",
                "        # See Example how a linear layer works for 3D input\r\n",
                "\r\n",
                "        qkv = qkv.reshape(\r\n",
                "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\r\n",
                "        )  # (n_samples, n_patches + 1, 3, n_heads, head_dim)\r\n",
                "\r\n",
                "        '''\r\n",
                "         EXPLANATION \r\n",
                "        ----------\r\n",
                "        qkv = torch.randn([4, 257, 3*768])\r\n",
                "        print(qkv.shape)\r\n",
                "\r\n",
                "        x = qkv.reshape(4, 257, 3, 12, 64)\r\n",
                "        print(x.shape)\r\n",
                "        '''   \r\n",
                " \r\n",
                "        qkv = qkv.permute(\r\n",
                "            2, 0, 3, 1, 4\r\n",
                "        ) # (3, n_samples, n_heads, n_patches + 1, head_dim)\r\n",
                "\r\n",
                "        q, k, v = qkv[0], qkv[1], qkv[2]\r\n",
                "\r\n",
                "        '''\r\n",
                "        EXPLANATION\r\n",
                "        -----------\r\n",
                "        qkv = qkv.permute(\r\n",
                "            2, 0, 3, 1, 4\r\n",
                "        ) \r\n",
                "        print(qkv.shape) --- > torch.Size([3, 4, 12, 257, 64])\r\n",
                "\r\n",
                "        q, k, v = qkv[0], qkv[1], qkv[2] \r\n",
                "        q.shape, k.shape, v.shape --- > torch.Size([4, 12, 257, 64]) - ALL 3\r\n",
                "        '''\r\n",
                "\r\n",
                "        key_transpose = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1) ---> torch.Size([4, 12, 64, 257])\r\n",
                "\r\n",
                "        dot_product = (\r\n",
                "            q @ key_transpose\r\n",
                "        ) * self.scale   # (n_samples, n_heads, n_patches + 1, n_patches + 1)\r\n",
                "\r\n",
                "        '''\r\n",
                "        q = torch.Size([4, 12, 257, 64]) \r\n",
                "        k.transpose = torch.Size([4, 12, 64, 257])\r\n",
                "        scale = (768/12) ** -0.5\r\n",
                "\r\n",
                "        dot_product ---> torch.Size([4, 12, 257, 257])\r\n",
                "        \r\n",
                "        '''\r\n",
                "        # attention = Softmax(Q.K_transpose)/ sqrt(dim of key vector)\r\n",
                "\r\n",
                "        attention = dot_product.softmax(dim = -1) \r\n",
                "        attention = self.attn_drop(attention)\r\n",
                "\r\n",
                "        weighted_avg = attention @ v  # (n_samples, n_headsm n_patches+1, head_dim)\r\n",
                "\r\n",
                "        weighted_avg = weighted_avg.transpose(1, 2) # (n_samples, n_patches +1, n_heads, head_dim)      \r\n",
                "\r\n",
                "        x = self.proj(weighted_avg)\r\n",
                "        x = self.proj_drop(x)   \r\n",
                "\r\n",
                "        return x "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class MLP(nn.Module):\r\n",
                "    \"\"\"\r\n",
                "    Multilayer Perceptron\r\n",
                "\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    in_features : int\r\n",
                "    hidden_features : int\r\n",
                "    out_features : int\r\n",
                "    p : float\r\n",
                "\r\n",
                "    Attribute\r\n",
                "    ---------\r\n",
                "    fc : nn.Linear\r\n",
                "    act : nn.GELU\r\n",
                "    fc2 : nn.Linear\r\n",
                "    drop : nn.Dropout\r\n",
                "    \"\"\"\r\n",
                "\r\n",
                "    def __init__(self, in_features, hidden_features, out_features, p = 0):\r\n",
                "        super().__init__()\r\n",
                "\r\n",
                "        self.fc1 = nn.Linear(in_features, hidden_features)\r\n",
                "        self.act = nn.GELU()\r\n",
                "        self.fc2 = nn.Linear(hidden_features, out_features)\r\n",
                "        self.drop = nn.Dropout(p)\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        \"\"\"\r\n",
                "        Run forward pass\r\n",
                "\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        x : torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, in_features)\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, out_features)'  Returns a shape 3 tensor\r\n",
                "        \"\"\"\r\n",
                "\r\n",
                "        x = self.fc1(x) # (n_samples, n_patches + 1, hidden_features)\r\n",
                "        x = self.act(x) \r\n",
                "        x = self.drop(x)\r\n",
                "\r\n",
                "        x = self.fc2(x) # (n_samples, n_patches + 1, out_features)\r\n",
                "        x = self.drop(x)\r\n",
                "        \r\n",
                "        return x"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "class Block(nn.Module):\r\n",
                "    '''\r\n",
                "    Transformer Block\r\n",
                "\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    dim : int\r\n",
                "        Embedding dimension\r\n",
                "\r\n",
                "    n_heads : int\r\n",
                "        Number of attention heads\r\n",
                "\r\n",
                "    mlp_ratio : float\r\n",
                "        Determines the hidden dimension size of the 'MLP' module with respect to 'dim'\r\n",
                "\r\n",
                "    qkv_bias : bool\r\n",
                "        If true then we include bias to the q,k,v projections\r\n",
                "\r\n",
                "    p, attn_p : float\r\n",
                "\r\n",
                "    Attributes\r\n",
                "    ----------\r\n",
                "    norm1, norm2 : LayerNorm\r\n",
                "\r\n",
                "    attn : Attention Module\r\n",
                "\r\n",
                "    mlp : MLP module\r\n",
                "    '''\r\n",
                "\r\n",
                "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\r\n",
                "        super().__init__()\r\n",
                "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6)\r\n",
                "\r\n",
                "        self.attn = Attention(\r\n",
                "            dim,\r\n",
                "            n_heads=n_heads,\r\n",
                "            qkv_bias=qkv_bias,\r\n",
                "            attn_p=attn_p,\r\n",
                "            proj_p=p\r\n",
                "        )\r\n",
                "\r\n",
                "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\r\n",
                "        hidden_features = int(dim * mlp_ratio)\r\n",
                "\r\n",
                "        self.mlp = MLP(\r\n",
                "            in_features=dim,\r\n",
                "            hidden_features=hidden_features,\r\n",
                "            out_features=dim\r\n",
                "        )\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        \"\"\"\r\n",
                "        Run forward pass\r\n",
                "\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        x : torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, dim)\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        torch.tensor\r\n",
                "            Shape '(n_samples, n_patches + 1, dim)'  Returns a shape 3 tensor\r\n",
                "        \"\"\"\r\n",
                "\r\n",
                "        x = x + self.attn(self.norm1(x))\r\n",
                "\r\n",
                "        x = x + self.mlp(self.norm2(x))\r\n",
                "\r\n",
                "        return x\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "class VisionTransformer(nn.Module):\r\n",
                "    \"\"\"\r\n",
                "    Simplified implementation of Vision Transformer\r\n",
                "\r\n",
                "    Parameters\r\n",
                "    ----------\r\n",
                "    img_size : int\r\n",
                "    patch_size : int\r\n",
                "    in_chans : int\r\n",
                "    n_classes : int\r\n",
                "\r\n",
                "    embed_dim : int\r\n",
                "        Dimensionality of the token/patch embeddings\r\n",
                "\r\n",
                "    depth : int\r\n",
                "        Number of Blocks\r\n",
                "\r\n",
                "    n_heads : int\r\n",
                "        Number of attention heads\r\n",
                "\r\n",
                "    mlp_ratio : float\r\n",
                "        Determines the hidden dimension of the 'MLP' module\r\n",
                "\r\n",
                "    qkv_bias : bool\r\n",
                "\r\n",
                "    p, attn_p : float\r\n",
                "        Dropout Probability\r\n",
                "\r\n",
                "    Attributes\r\n",
                "    ----------\r\n",
                "    patch_embed : PatchEmbedding\r\n",
                "        Instance of 'PatchEmbedding' layer\r\n",
                "\r\n",
                "    cls_token : nn.Parameter\r\n",
                "        Learnable parameter that will represent the first token in the sequence\r\n",
                "        It has embed_dim elements\r\n",
                "\r\n",
                "    pos_embed : nn.Parameter\r\n",
                "        positional embedding of the cls token + all the patches\r\n",
                "        It has '(n_patches + 1) * embed_dim' elements\r\n",
                "\r\n",
                "    pos_drop = nn.Dropout\r\n",
                "        Dropout Layer\r\n",
                "\r\n",
                "    blocks : nn.ModuleList\r\n",
                "        List of 'Block' modules\r\n",
                "\r\n",
                "    norm : nn.LayerNorm\r\n",
                "        Layer Normalisation\r\n",
                "\r\n",
                "    \"\"\"\r\n",
                "\r\n",
                "    def __init__(self,\r\n",
                "                img_size=384,\r\n",
                "                patch_size=16,\r\n",
                "                in_chans=3,\r\n",
                "                n_classes=1000,\r\n",
                "                embed_dim=768,\r\n",
                "                depth=12,\r\n",
                "                n_heads=12,\r\n",
                "                mlp_ratio=4,\r\n",
                "                qkv_bias=True,\r\n",
                "                p=0.,\r\n",
                "                attn_p=0.,\r\n",
                "    ):\r\n",
                "\r\n",
                "        super().__init__()\r\n",
                "\r\n",
                "        self.patch_embed = PatchEmbedding(\r\n",
                "                    img_size=img_size,\r\n",
                "                    patch_size=patch_size,\r\n",
                "                    in_chans=in_chans,\r\n",
                "                    embed_dim=embed_dim\r\n",
                "        )\r\n",
                "\r\n",
                "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\r\n",
                "        \r\n",
                "        self.pos_embed = nn.Parameter(\r\n",
                "                torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\r\n",
                "        )\r\n",
                "\r\n",
                "        self.pos_drop = nn.Dropout(p=p)\r\n",
                "\r\n",
                "        self.blocks = nn.ModuleList(\r\n",
                "            [\r\n",
                "                Block(\r\n",
                "                    dim=embed_dim,\r\n",
                "                    n_heads=n_heads,\r\n",
                "                    mlp_ratio=mlp_ratio,\r\n",
                "                    qkv_bias=qkv_bias,\r\n",
                "                    p=p,\r\n",
                "                    attn_p=attn_p\r\n",
                "                    )\r\n",
                "\r\n",
                "                for _ in range(depth)\r\n",
                "            ]\r\n",
                "        )\r\n",
                "\r\n",
                "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\r\n",
                "\r\n",
                "        self.head = nn.Linear(embed_dim, n_classes)\r\n",
                "\r\n",
                "    def forward(self, x):\r\n",
                "        ''' Run the forward pass\r\n",
                "        \r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        x : torch.tensor\r\n",
                "            Shape '(n_samples, in_chans, img_size, img_size)'\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        logits : torch.tensor\r\n",
                "        '''\r\n",
                "\r\n",
                "        n_samples = x.shape[0]\r\n",
                "        x = self.patch_embed(x)\r\n",
                "\r\n",
                "        cls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, embed_dim)\r\n",
                "\r\n",
                "        x = torch.cat((cls_token, x), dim=1) # (n_samples, 1 + n_patches, embed_dim)\r\n",
                "\r\n",
                "        x = x + self.pos_embed # (n_samples, 1 + n_patches, embed_dim)\r\n",
                "        x = self.pos_drop(x)\r\n",
                "\r\n",
                "        for block in self.blocks:\r\n",
                "            x = block(x)\r\n",
                "\r\n",
                "        x = self.norm(x)\r\n",
                "\r\n",
                "        cls_token_final = x[:, 0] # Just the CLS token\r\n",
                "\r\n",
                "        x = self.head(cls_token_final)\r\n",
                "\r\n",
                "        return x"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "def get_n_params(module):\r\n",
                "    return sum(p.numel() for p in module.parameters() if p.requires_grad)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "import timm\r\n",
                "\r\n",
                "model_name = \"vit_base_patch16_384\"\r\n",
                "model_official = timm.create_model(model_name, pretrained=True)\r\n",
                "model_official.eval()\r\n",
                "print(type(model_official))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "<class 'timm.models.vision_transformer.VisionTransformer'>\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "custom_config = {\r\n",
                "    \"img_size\":384,\r\n",
                "    \"in_chans\" : 3,\r\n",
                "    \"patch_size\" : 16,\r\n",
                "    \"embed_dim\" : 768, \r\n",
                "    \"depth\" : 12,\r\n",
                "    \"n_heads\" : 12,\r\n",
                "    \"qkv_bias\" : True,\r\n",
                "    \"mlp_ratio\" : 4\r\n",
                "}\r\n",
                "\r\n",
                "model_custom = VisionTransformer(**custom_config)\r\n",
                "model_custom.eval()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "import numpy as np\r\n",
                "\r\n",
                "def assert_tensors_equal(t1, t2):\r\n",
                "    a1, a2 = t1.detach().cpu().numpy(), t2.detach().cpu().numpy()\r\n",
                "\r\n",
                "    np.testing.assert_allclose(a1, a2)\r\n",
                "\r\n",
                "for (n_o, p_o) , (n_c, p_c) in zip(\r\n",
                "    model_official.named_parameters(), model_custom.named_parameters()\r\n",
                "):\r\n",
                "    assert p_o.numel() == p_c.numel()\r\n",
                "    print(f\"{n_o} | {n_c}\")\r\n",
                "\r\n",
                "    if (n_o != n_c):\r\n",
                "        print(\"Variable names are diff for - \", n_o , n_c)\r\n",
                "\r\n",
                "    p_c.data[:] = p_o.data\r\n",
                "\r\n",
                "    assert_tensors_equal(p_c.data, p_o.data)\r\n",
                "\r\n",
                "inp = torch.rand(1, 3, 384, 384)\r\n",
                "res_c = model_custom(inp.cuda())\r\n",
                "res_o = model_official(inp.cuda())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "cls_token | cls_token\n",
                        "pos_embed | pos_embed\n",
                        "patch_embed.proj.weight | patch_embed.proj.weight\n",
                        "patch_embed.proj.bias | patch_embed.proj.bias\n",
                        "blocks.0.norm1.weight | blocks.0.norm1.weight\n",
                        "blocks.0.norm1.bias | blocks.0.norm1.bias\n",
                        "blocks.0.attn.qkv.weight | blocks.0.attn.qkv.weight\n",
                        "blocks.0.attn.qkv.bias | blocks.0.attn.qkv.bias\n",
                        "blocks.0.attn.proj.weight | blocks.0.attn.proj.weight\n",
                        "blocks.0.attn.proj.bias | blocks.0.attn.proj.bias\n",
                        "blocks.0.norm2.weight | blocks.0.norm2.weight\n",
                        "blocks.0.norm2.bias | blocks.0.norm2.bias\n",
                        "blocks.0.mlp.fc1.weight | blocks.0.mlp.fc1.weight\n",
                        "blocks.0.mlp.fc1.bias | blocks.0.mlp.fc1.bias\n",
                        "blocks.0.mlp.fc2.weight | blocks.0.mlp.fc2.weight\n",
                        "blocks.0.mlp.fc2.bias | blocks.0.mlp.fc2.bias\n",
                        "blocks.1.norm1.weight | blocks.1.norm1.weight\n",
                        "blocks.1.norm1.bias | blocks.1.norm1.bias\n",
                        "blocks.1.attn.qkv.weight | blocks.1.attn.qkv.weight\n",
                        "blocks.1.attn.qkv.bias | blocks.1.attn.qkv.bias\n",
                        "blocks.1.attn.proj.weight | blocks.1.attn.proj.weight\n",
                        "blocks.1.attn.proj.bias | blocks.1.attn.proj.bias\n",
                        "blocks.1.norm2.weight | blocks.1.norm2.weight\n",
                        "blocks.1.norm2.bias | blocks.1.norm2.bias\n",
                        "blocks.1.mlp.fc1.weight | blocks.1.mlp.fc1.weight\n",
                        "blocks.1.mlp.fc1.bias | blocks.1.mlp.fc1.bias\n",
                        "blocks.1.mlp.fc2.weight | blocks.1.mlp.fc2.weight\n",
                        "blocks.1.mlp.fc2.bias | blocks.1.mlp.fc2.bias\n",
                        "blocks.2.norm1.weight | blocks.2.norm1.weight\n",
                        "blocks.2.norm1.bias | blocks.2.norm1.bias\n",
                        "blocks.2.attn.qkv.weight | blocks.2.attn.qkv.weight\n",
                        "blocks.2.attn.qkv.bias | blocks.2.attn.qkv.bias\n",
                        "blocks.2.attn.proj.weight | blocks.2.attn.proj.weight\n",
                        "blocks.2.attn.proj.bias | blocks.2.attn.proj.bias\n",
                        "blocks.2.norm2.weight | blocks.2.norm2.weight\n",
                        "blocks.2.norm2.bias | blocks.2.norm2.bias\n",
                        "blocks.2.mlp.fc1.weight | blocks.2.mlp.fc1.weight\n",
                        "blocks.2.mlp.fc1.bias | blocks.2.mlp.fc1.bias\n",
                        "blocks.2.mlp.fc2.weight | blocks.2.mlp.fc2.weight\n",
                        "blocks.2.mlp.fc2.bias | blocks.2.mlp.fc2.bias\n",
                        "blocks.3.norm1.weight | blocks.3.norm1.weight\n",
                        "blocks.3.norm1.bias | blocks.3.norm1.bias\n",
                        "blocks.3.attn.qkv.weight | blocks.3.attn.qkv.weight\n",
                        "blocks.3.attn.qkv.bias | blocks.3.attn.qkv.bias\n",
                        "blocks.3.attn.proj.weight | blocks.3.attn.proj.weight\n",
                        "blocks.3.attn.proj.bias | blocks.3.attn.proj.bias\n",
                        "blocks.3.norm2.weight | blocks.3.norm2.weight\n",
                        "blocks.3.norm2.bias | blocks.3.norm2.bias\n",
                        "blocks.3.mlp.fc1.weight | blocks.3.mlp.fc1.weight\n",
                        "blocks.3.mlp.fc1.bias | blocks.3.mlp.fc1.bias\n",
                        "blocks.3.mlp.fc2.weight | blocks.3.mlp.fc2.weight\n",
                        "blocks.3.mlp.fc2.bias | blocks.3.mlp.fc2.bias\n",
                        "blocks.4.norm1.weight | blocks.4.norm1.weight\n",
                        "blocks.4.norm1.bias | blocks.4.norm1.bias\n",
                        "blocks.4.attn.qkv.weight | blocks.4.attn.qkv.weight\n",
                        "blocks.4.attn.qkv.bias | blocks.4.attn.qkv.bias\n",
                        "blocks.4.attn.proj.weight | blocks.4.attn.proj.weight\n",
                        "blocks.4.attn.proj.bias | blocks.4.attn.proj.bias\n",
                        "blocks.4.norm2.weight | blocks.4.norm2.weight\n",
                        "blocks.4.norm2.bias | blocks.4.norm2.bias\n",
                        "blocks.4.mlp.fc1.weight | blocks.4.mlp.fc1.weight\n",
                        "blocks.4.mlp.fc1.bias | blocks.4.mlp.fc1.bias\n",
                        "blocks.4.mlp.fc2.weight | blocks.4.mlp.fc2.weight\n",
                        "blocks.4.mlp.fc2.bias | blocks.4.mlp.fc2.bias\n",
                        "blocks.5.norm1.weight | blocks.5.norm1.weight\n",
                        "blocks.5.norm1.bias | blocks.5.norm1.bias\n",
                        "blocks.5.attn.qkv.weight | blocks.5.attn.qkv.weight\n",
                        "blocks.5.attn.qkv.bias | blocks.5.attn.qkv.bias\n",
                        "blocks.5.attn.proj.weight | blocks.5.attn.proj.weight\n",
                        "blocks.5.attn.proj.bias | blocks.5.attn.proj.bias\n",
                        "blocks.5.norm2.weight | blocks.5.norm2.weight\n",
                        "blocks.5.norm2.bias | blocks.5.norm2.bias\n",
                        "blocks.5.mlp.fc1.weight | blocks.5.mlp.fc1.weight\n",
                        "blocks.5.mlp.fc1.bias | blocks.5.mlp.fc1.bias\n",
                        "blocks.5.mlp.fc2.weight | blocks.5.mlp.fc2.weight\n",
                        "blocks.5.mlp.fc2.bias | blocks.5.mlp.fc2.bias\n",
                        "blocks.6.norm1.weight | blocks.6.norm1.weight\n",
                        "blocks.6.norm1.bias | blocks.6.norm1.bias\n",
                        "blocks.6.attn.qkv.weight | blocks.6.attn.qkv.weight\n",
                        "blocks.6.attn.qkv.bias | blocks.6.attn.qkv.bias\n",
                        "blocks.6.attn.proj.weight | blocks.6.attn.proj.weight\n",
                        "blocks.6.attn.proj.bias | blocks.6.attn.proj.bias\n",
                        "blocks.6.norm2.weight | blocks.6.norm2.weight\n",
                        "blocks.6.norm2.bias | blocks.6.norm2.bias\n",
                        "blocks.6.mlp.fc1.weight | blocks.6.mlp.fc1.weight\n",
                        "blocks.6.mlp.fc1.bias | blocks.6.mlp.fc1.bias\n",
                        "blocks.6.mlp.fc2.weight | blocks.6.mlp.fc2.weight\n",
                        "blocks.6.mlp.fc2.bias | blocks.6.mlp.fc2.bias\n",
                        "blocks.7.norm1.weight | blocks.7.norm1.weight\n",
                        "blocks.7.norm1.bias | blocks.7.norm1.bias\n",
                        "blocks.7.attn.qkv.weight | blocks.7.attn.qkv.weight\n",
                        "blocks.7.attn.qkv.bias | blocks.7.attn.qkv.bias\n",
                        "blocks.7.attn.proj.weight | blocks.7.attn.proj.weight\n",
                        "blocks.7.attn.proj.bias | blocks.7.attn.proj.bias\n",
                        "blocks.7.norm2.weight | blocks.7.norm2.weight\n",
                        "blocks.7.norm2.bias | blocks.7.norm2.bias\n",
                        "blocks.7.mlp.fc1.weight | blocks.7.mlp.fc1.weight\n",
                        "blocks.7.mlp.fc1.bias | blocks.7.mlp.fc1.bias\n",
                        "blocks.7.mlp.fc2.weight | blocks.7.mlp.fc2.weight\n",
                        "blocks.7.mlp.fc2.bias | blocks.7.mlp.fc2.bias\n",
                        "blocks.8.norm1.weight | blocks.8.norm1.weight\n",
                        "blocks.8.norm1.bias | blocks.8.norm1.bias\n",
                        "blocks.8.attn.qkv.weight | blocks.8.attn.qkv.weight\n",
                        "blocks.8.attn.qkv.bias | blocks.8.attn.qkv.bias\n",
                        "blocks.8.attn.proj.weight | blocks.8.attn.proj.weight\n",
                        "blocks.8.attn.proj.bias | blocks.8.attn.proj.bias\n",
                        "blocks.8.norm2.weight | blocks.8.norm2.weight\n",
                        "blocks.8.norm2.bias | blocks.8.norm2.bias\n",
                        "blocks.8.mlp.fc1.weight | blocks.8.mlp.fc1.weight\n",
                        "blocks.8.mlp.fc1.bias | blocks.8.mlp.fc1.bias\n",
                        "blocks.8.mlp.fc2.weight | blocks.8.mlp.fc2.weight\n",
                        "blocks.8.mlp.fc2.bias | blocks.8.mlp.fc2.bias\n",
                        "blocks.9.norm1.weight | blocks.9.norm1.weight\n",
                        "blocks.9.norm1.bias | blocks.9.norm1.bias\n",
                        "blocks.9.attn.qkv.weight | blocks.9.attn.qkv.weight\n",
                        "blocks.9.attn.qkv.bias | blocks.9.attn.qkv.bias\n",
                        "blocks.9.attn.proj.weight | blocks.9.attn.proj.weight\n",
                        "blocks.9.attn.proj.bias | blocks.9.attn.proj.bias\n",
                        "blocks.9.norm2.weight | blocks.9.norm2.weight\n",
                        "blocks.9.norm2.bias | blocks.9.norm2.bias\n",
                        "blocks.9.mlp.fc1.weight | blocks.9.mlp.fc1.weight\n",
                        "blocks.9.mlp.fc1.bias | blocks.9.mlp.fc1.bias\n",
                        "blocks.9.mlp.fc2.weight | blocks.9.mlp.fc2.weight\n",
                        "blocks.9.mlp.fc2.bias | blocks.9.mlp.fc2.bias\n",
                        "blocks.10.norm1.weight | blocks.10.norm1.weight\n",
                        "blocks.10.norm1.bias | blocks.10.norm1.bias\n",
                        "blocks.10.attn.qkv.weight | blocks.10.attn.qkv.weight\n",
                        "blocks.10.attn.qkv.bias | blocks.10.attn.qkv.bias\n",
                        "blocks.10.attn.proj.weight | blocks.10.attn.proj.weight\n",
                        "blocks.10.attn.proj.bias | blocks.10.attn.proj.bias\n",
                        "blocks.10.norm2.weight | blocks.10.norm2.weight\n",
                        "blocks.10.norm2.bias | blocks.10.norm2.bias\n",
                        "blocks.10.mlp.fc1.weight | blocks.10.mlp.fc1.weight\n",
                        "blocks.10.mlp.fc1.bias | blocks.10.mlp.fc1.bias\n",
                        "blocks.10.mlp.fc2.weight | blocks.10.mlp.fc2.weight\n",
                        "blocks.10.mlp.fc2.bias | blocks.10.mlp.fc2.bias\n",
                        "blocks.11.norm1.weight | blocks.11.norm1.weight\n",
                        "blocks.11.norm1.bias | blocks.11.norm1.bias\n",
                        "blocks.11.attn.qkv.weight | blocks.11.attn.qkv.weight\n",
                        "blocks.11.attn.qkv.bias | blocks.11.attn.qkv.bias\n",
                        "blocks.11.attn.proj.weight | blocks.11.attn.proj.weight\n",
                        "blocks.11.attn.proj.bias | blocks.11.attn.proj.bias\n",
                        "blocks.11.norm2.weight | blocks.11.norm2.weight\n",
                        "blocks.11.norm2.bias | blocks.11.norm2.bias\n",
                        "blocks.11.mlp.fc1.weight | blocks.11.mlp.fc1.weight\n",
                        "blocks.11.mlp.fc1.bias | blocks.11.mlp.fc1.bias\n",
                        "blocks.11.mlp.fc2.weight | blocks.11.mlp.fc2.weight\n",
                        "blocks.11.mlp.fc2.bias | blocks.11.mlp.fc2.bias\n",
                        "norm.weight | norm.weight\n",
                        "norm.bias | norm.bias\n",
                        "head.weight | head.weight\n",
                        "head.bias | head.bias\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "RuntimeError",
                    "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/1921959741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m384\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m384\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mres_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_custom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mres_o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_official\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2484598000.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/3297659334.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2558600921.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mweighted_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (n_samples, n_patches +1, n_heads, head_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproj_drop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from torchsummary import summary\r\n",
                "\r\n",
                "summary(model_official.cuda(), (3, 384, 384))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import torch\r\n",
                "qkv = torch.randn([4, 257, 3*768])\r\n",
                "print(qkv.shape)\r\n",
                "\r\n",
                "qkv = qkv.reshape(4, 257, 3, 12, 64)\r\n",
                "\r\n",
                "qkv = qkv.permute(\r\n",
                "            2, 0, 3, 1, 4\r\n",
                "        ) \r\n",
                "print(qkv.shape)\r\n",
                "q, k, v = qkv[0], qkv[1], qkv[2] \r\n",
                "q.shape, k.shape, v.shape"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([4, 257, 2304])\n",
                        "torch.Size([3, 4, 12, 257, 64])\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(torch.Size([4, 12, 257, 64]),\n",
                            " torch.Size([4, 12, 257, 64]),\n",
                            " torch.Size([4, 12, 257, 64]))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "key_transpose = k.transpose(-2, -1)\r\n",
                "key_transpose.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "torch.Size([4, 12, 64, 257])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "scale = 64 ** -0.5\r\n",
                "dot_product = (\r\n",
                "            q @ key_transpose\r\n",
                "        ) * scale \r\n",
                "\r\n",
                "print(q.shape, key_transpose.shape)\r\n",
                "dot_product.shape"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([4, 12, 257, 64]) torch.Size([4, 12, 64, 257])\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "torch.Size([4, 12, 257, 257])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "x = torch.ones([3, 3])\r\n",
                "y = torch.ones([5, 3])\r\n",
                "\r\n",
                "z = x @ y.transpose(-1, 0)\r\n",
                "print(z.shape)\r\n",
                "z"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([3, 5])\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "tensor([[3., 3., 3., 3., 3.],\n",
                            "        [3., 3., 3., 3., 3.],\n",
                            "        [3., 3., 3., 3., 3.]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 18
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "y = y.transpose(-1, 0)\r\n",
                "y.shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "torch.Size([3, 5])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.5 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "af4eb978952372bacf930848cffd184ec188160e0cc994a600acaa7882bf4888"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}